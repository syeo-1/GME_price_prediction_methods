{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>RSI</th>\n",
       "      <th>SMA</th>\n",
       "      <th>LMA</th>\n",
       "      <th>ADX</th>\n",
       "      <th>up</th>\n",
       "      <th>Score</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-02-17</td>\n",
       "      <td>49.770000</td>\n",
       "      <td>51.189999</td>\n",
       "      <td>44.560001</td>\n",
       "      <td>45.939999</td>\n",
       "      <td>45.939999</td>\n",
       "      <td>9186800.0</td>\n",
       "      <td>45.962590</td>\n",
       "      <td>110.887913</td>\n",
       "      <td>78.886475</td>\n",
       "      <td>26.642816</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.173827</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-02-17</td>\n",
       "      <td>49.752222</td>\n",
       "      <td>51.157777</td>\n",
       "      <td>44.505695</td>\n",
       "      <td>45.867082</td>\n",
       "      <td>45.867082</td>\n",
       "      <td>9186800.0</td>\n",
       "      <td>45.953698</td>\n",
       "      <td>110.590784</td>\n",
       "      <td>78.899723</td>\n",
       "      <td>26.625878</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.173827</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-02-17</td>\n",
       "      <td>49.734445</td>\n",
       "      <td>51.125555</td>\n",
       "      <td>44.451390</td>\n",
       "      <td>45.794166</td>\n",
       "      <td>45.794166</td>\n",
       "      <td>9186800.0</td>\n",
       "      <td>45.944800</td>\n",
       "      <td>110.295705</td>\n",
       "      <td>78.912937</td>\n",
       "      <td>26.608974</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.173827</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-02-17</td>\n",
       "      <td>49.716667</td>\n",
       "      <td>51.093332</td>\n",
       "      <td>44.397084</td>\n",
       "      <td>45.721249</td>\n",
       "      <td>45.721249</td>\n",
       "      <td>9186800.0</td>\n",
       "      <td>45.935898</td>\n",
       "      <td>110.002673</td>\n",
       "      <td>78.926115</td>\n",
       "      <td>26.592103</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.173827</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-02-17</td>\n",
       "      <td>49.698889</td>\n",
       "      <td>51.061110</td>\n",
       "      <td>44.342779</td>\n",
       "      <td>45.648332</td>\n",
       "      <td>45.648332</td>\n",
       "      <td>9186800.0</td>\n",
       "      <td>45.926990</td>\n",
       "      <td>109.711690</td>\n",
       "      <td>78.939260</td>\n",
       "      <td>26.575264</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.173827</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date       Open       High        Low      Close  Adj Close  \\\n",
       "0  2021-02-17  49.770000  51.189999  44.560001  45.939999  45.939999   \n",
       "1  2021-02-17  49.752222  51.157777  44.505695  45.867082  45.867082   \n",
       "2  2021-02-17  49.734445  51.125555  44.451390  45.794166  45.794166   \n",
       "3  2021-02-17  49.716667  51.093332  44.397084  45.721249  45.721249   \n",
       "4  2021-02-17  49.698889  51.061110  44.342779  45.648332  45.648332   \n",
       "\n",
       "      Volume        RSI         SMA        LMA        ADX  up     Score  \\\n",
       "0  9186800.0  45.962590  110.887913  78.886475  26.642816   0 -0.173827   \n",
       "1  9186800.0  45.953698  110.590784  78.899723  26.625878   0 -0.173827   \n",
       "2  9186800.0  45.944800  110.295705  78.912937  26.608974   0 -0.173827   \n",
       "3  9186800.0  45.935898  110.002673  78.926115  26.592103   0 -0.173827   \n",
       "4  9186800.0  45.926990  109.711690  78.939260  26.575264   0 -0.173827   \n",
       "\n",
       "   Sentiment  \n",
       "0         -1  \n",
       "1         -1  \n",
       "2         -1  \n",
       "3         -1  \n",
       "4         -1  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load up the augmented processed GME data\n",
    "processed_gme_data = pd.read_csv('GME_augmented_processed.csv')\n",
    "processed_gme_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing to do is to process the data for the LSTM. We'll have to create windows of data to feed into the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need 'lag obeservations': https://machinelearningmastery.com/how-to-use-the-timeseriesgenerator-for-time-series-forecasting-in-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "<keras.preprocessing.sequence.TimeseriesGenerator object at 0x1483e0520>\n",
      "[[1 2]] => [3]\n",
      "[[2 3]] => [4]\n",
      "[[3 4]] => [5]\n",
      "[[4 5]] => [6]\n",
      "[[5 6]] => [7]\n",
      "[[6 7]] => [8]\n",
      "[[7 8]] => [9]\n",
      "[[8 9]] => [10]\n"
     ]
    }
   ],
   "source": [
    "# dummy example\n",
    "series = np.array([1,2,3,4,5,6,7,8,9,10])\n",
    "n_input = 2\n",
    "generator = TimeseriesGenerator(series, series, length=n_input, batch_size=1)\n",
    "print(len(generator))\n",
    "# for i in range(len(generator)):\n",
    "print(generator) \n",
    "for i in range(len(generator)):\n",
    "    x, y = generator[i]\n",
    "    print('%s => %s' % (x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109.0\n",
      "5493\n"
     ]
    }
   ],
   "source": [
    "# data is for 109 days\n",
    "print(len(processed_gme_data)/72)\n",
    "\n",
    "# get the split point\n",
    "split_point = int(len(processed_gme_data)*0.7)\n",
    "print(split_point)\n",
    "\n",
    "# create a list of start of day indices to find the closest start of day index\n",
    "start_of_day_indices = [i for i in range(0, len(processed_gme_data), 72)]\n",
    "\n",
    "# find the start of day index that's closest to the split point\n",
    "min_dist = math.inf\n",
    "split_index = None\n",
    "for start_of_day_index in start_of_day_indices:\n",
    "    if abs(split_point-start_of_day_index) < min_dist:\n",
    "        min_dist = abs(split_point-start_of_day_index)\n",
    "        split_index = start_of_day_index\n",
    "# print(f'{split_index} represents day {index_to_use/72}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, training data will be using data from all days up to the 76th day (exclusive). In other words, training data uses 75 \"full\" days worth of data. Testing will use the rest of the data starting from the 76th day and onwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a window size for the length parameter (to use for TimeseriesGenerator later)\n",
    "window_size = 1\n",
    "\n",
    "# separate the data to training and testing (approximately 70/30)\n",
    "training = processed_gme_data[:split_index]\n",
    "# offset by window_size since time series training will use input of window_size rows with output of the first row\n",
    "testing = processed_gme_data[split_index - window_size:]\n",
    "\n",
    "# separate each data group into data and labels\n",
    "training_data = training.loc[:, 'Open':'up']\n",
    "# training_data = training.loc[:, 'Open':'ADX']\n",
    "# training_labels = training.loc[:, 'up']\n",
    "\n",
    "testing_data = testing.loc[:, 'Open':'up']\n",
    "# testing_data = testing.loc[:, 'Open':'ADX']\n",
    "# testing_labels = testing.loc[:, 'up']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>RSI</th>\n",
       "      <th>SMA</th>\n",
       "      <th>LMA</th>\n",
       "      <th>ADX</th>\n",
       "      <th>up</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49.770000</td>\n",
       "      <td>51.189999</td>\n",
       "      <td>44.560001</td>\n",
       "      <td>45.939999</td>\n",
       "      <td>45.939999</td>\n",
       "      <td>9186800.0</td>\n",
       "      <td>45.962590</td>\n",
       "      <td>110.887913</td>\n",
       "      <td>78.886475</td>\n",
       "      <td>26.642816</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49.752222</td>\n",
       "      <td>51.157777</td>\n",
       "      <td>44.505695</td>\n",
       "      <td>45.867082</td>\n",
       "      <td>45.867082</td>\n",
       "      <td>9186800.0</td>\n",
       "      <td>45.953698</td>\n",
       "      <td>110.590784</td>\n",
       "      <td>78.899723</td>\n",
       "      <td>26.625878</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49.734445</td>\n",
       "      <td>51.125555</td>\n",
       "      <td>44.451390</td>\n",
       "      <td>45.794166</td>\n",
       "      <td>45.794166</td>\n",
       "      <td>9186800.0</td>\n",
       "      <td>45.944800</td>\n",
       "      <td>110.295705</td>\n",
       "      <td>78.912937</td>\n",
       "      <td>26.608974</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49.716667</td>\n",
       "      <td>51.093332</td>\n",
       "      <td>44.397084</td>\n",
       "      <td>45.721249</td>\n",
       "      <td>45.721249</td>\n",
       "      <td>9186800.0</td>\n",
       "      <td>45.935898</td>\n",
       "      <td>110.002673</td>\n",
       "      <td>78.926115</td>\n",
       "      <td>26.592103</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49.698889</td>\n",
       "      <td>51.061110</td>\n",
       "      <td>44.342779</td>\n",
       "      <td>45.648332</td>\n",
       "      <td>45.648332</td>\n",
       "      <td>9186800.0</td>\n",
       "      <td>45.926990</td>\n",
       "      <td>109.711690</td>\n",
       "      <td>78.939260</td>\n",
       "      <td>26.575264</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Open       High        Low      Close  Adj Close     Volume  \\\n",
       "0  49.770000  51.189999  44.560001  45.939999  45.939999  9186800.0   \n",
       "1  49.752222  51.157777  44.505695  45.867082  45.867082  9186800.0   \n",
       "2  49.734445  51.125555  44.451390  45.794166  45.794166  9186800.0   \n",
       "3  49.716667  51.093332  44.397084  45.721249  45.721249  9186800.0   \n",
       "4  49.698889  51.061110  44.342779  45.648332  45.648332  9186800.0   \n",
       "\n",
       "         RSI         SMA        LMA        ADX  up  \n",
       "0  45.962590  110.887913  78.886475  26.642816   0  \n",
       "1  45.953698  110.590784  78.899723  26.625878   0  \n",
       "2  45.944800  110.295705  78.912937  26.608974   0  \n",
       "3  45.935898  110.002673  78.926115  26.592103   0  \n",
       "4  45.926990  109.711690  78.939260  26.575264   0  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize training/testing data using standardscaler\n",
    "# this also converts the data into numpy arrays (necessary for TimeseriesGenerator)\n",
    "data_scaler = StandardScaler()\n",
    "normalized_training_data = data_scaler.fit_transform(training_data)\n",
    "# training_data_scaler = StandardScaler()\n",
    "# training_data_scaler = training_data_scaler.fit(training_data)\n",
    "# normalized_training_data = training_data_scaler.transform(training_data)\n",
    "# normalized_training_data\n",
    "\n",
    "# testing_data_scaler = StandardScaler()\n",
    "# testing_data_scaler = testing_data_scaler.fit(testing_data)\n",
    "# normalized_testing_data = testing_data_scaler.transform(testing_data)\n",
    "normalized_testing_data = data_scaler.transform(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.97402153 -0.97402153 -0.97402153 ...  1.02667135  1.02667135\n",
      "  1.02667135]\n",
      "[1.02667135 1.02667135 1.02667135 ... 1.02667135 1.02667135 1.02667135]\n"
     ]
    }
   ],
   "source": [
    "# identify the label columns\n",
    "normalized_training_labels = normalized_training_data[:, 10]\n",
    "normalized_testing_labels = normalized_testing_data[:, 10]\n",
    "print(normalized_training_labels)\n",
    "print(normalized_testing_labels)\n",
    "# training_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# testing_ts_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(normalized_testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dataframe slices to numpy arrays for the timeseries generator\n",
    "# training_data = np.array(training_data)\n",
    "# training_labels = np.array(training_labels)\n",
    "# testing_data = np.array(testing_data)\n",
    "# testing_labels = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create timeseries generators for training and testing data/labels\n",
    "training_ts_generator = TimeseriesGenerator(\n",
    "    data=normalized_training_data,\n",
    "    targets=normalized_training_labels,\n",
    "#     targets=training_labels,\n",
    "    length=window_size,\n",
    "    batch_size=1\n",
    ")\n",
    "\n",
    "testing_ts_generator = TimeseriesGenerator(\n",
    "    data=normalized_testing_data,\n",
    "    targets=normalized_testing_labels,\n",
    "#     targets=testing_labels,\n",
    "    length=window_size,\n",
    "    batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_gme_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-2.37120045 -2.40507799 -2.3988217  -2.45805618 -2.45805618\n",
      "   -0.4053415  -1.059327   -1.01771832 -2.04087564  0.81276943\n",
      "   -0.97402153]]] => [-0.97402153]\n"
     ]
    }
   ],
   "source": [
    "# just an example to see what output is\n",
    "for i in range(len(training_ts_generator)):\n",
    "    x, y = training_ts_generator[i]\n",
    "    print('%s => %s' % (x, y))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1.70357314  1.72573509  2.02874408  2.11192245  2.11192245\n",
      "   -0.6397665   1.38802248  1.43679133  1.17929748  1.33863748\n",
      "    1.02667135]]] => [1.02667135]\n"
     ]
    }
   ],
   "source": [
    "# just an example to see what output is\n",
    "for i in range(len(testing_ts_generator)):\n",
    "    x, y = testing_ts_generator[i]\n",
    "    print('%s => %s' % (x, y))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized_testing_data.shape\n",
    "# training_ts_generator.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, 1, 64)             19456     \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 96)                61824     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 81,377\n",
      "Trainable params: 81,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define an LSTM for the normalized training data\n",
    "training_model = Sequential()\n",
    "# input_shape for each data input is window_size x number of features (column) in training_data\n",
    "training_model.add(LSTM(64, activation='sigmoid', input_shape=(window_size, 11), return_sequences=True))\n",
    "training_model.add(LSTM(96, activation='sigmoid', return_sequences=False))\n",
    "training_model.add(Dropout(0.2))\n",
    "\n",
    "# output either standardized 0 or 1\n",
    "training_model.add(Dense(1))\n",
    "\n",
    "training_model.compile(optimizer='SGD', loss='mse')\n",
    "training_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5471/5471 [==============================] - 23s 4ms/step - loss: 1.0059 - val_loss: 0.0495\n",
      "CPU times: user 34 s, sys: 3.59 s, total: 37.6 s\n",
      "Wall time: 23.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history_train = training_model.fit(training_ts_generator, validation_data=testing_ts_generator, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.6804473400115967,\n",
       "  0.10076028853654861,\n",
       "  0.07144468277692795,\n",
       "  0.06159302219748497,\n",
       "  0.057770390063524246,\n",
       "  0.05392524227499962,\n",
       "  0.05141257494688034,\n",
       "  0.048393603414297104,\n",
       "  0.04924182593822479,\n",
       "  0.04562942683696747],\n",
       " 'val_loss': [0.05545632168650627,\n",
       "  0.03688159957528114,\n",
       "  0.028984947130084038,\n",
       "  0.03040388599038124,\n",
       "  0.028940264135599136,\n",
       "  0.028623705729842186,\n",
       "  0.027474328875541687,\n",
       "  0.0458141528069973,\n",
       "  0.027035757899284363,\n",
       "  0.029883403331041336]}"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_train.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# history_train.history\n",
    "plt.plot(history_train.history['loss'])\n",
    "plt.plot(history_train.history['val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5471/5471 [==============================] - 9s 2ms/step - loss: 0.0299\n",
      "2376/2376 [==============================] - 5s 2ms/step - loss: 0.0299\n"
     ]
    }
   ],
   "source": [
    "training_results = training_model.evaluate(training_ts_generator)\n",
    "testing_results = training_model.evaluate(testing_ts_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results\n",
    "prediction_results = training_model.predict(testing_ts_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.014667 ],\n",
       "       [1.014672 ],\n",
       "       [1.0146439],\n",
       "       ...,\n",
       "       [1.0088737],\n",
       "       [1.0088971],\n",
       "       [1.0089209]], dtype=float32)"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.014667 , 1.014667 , 1.014667 , ..., 1.014667 , 1.014667 ,\n",
       "        1.014667 ],\n",
       "       [1.014672 , 1.014672 , 1.014672 , ..., 1.014672 , 1.014672 ,\n",
       "        1.014672 ],\n",
       "       [1.0146439, 1.0146439, 1.0146439, ..., 1.0146439, 1.0146439,\n",
       "        1.0146439],\n",
       "       ...,\n",
       "       [1.0088737, 1.0088737, 1.0088737, ..., 1.0088737, 1.0088737,\n",
       "        1.0088737],\n",
       "       [1.0088971, 1.0088971, 1.0088971, ..., 1.0088971, 1.0088971,\n",
       "        1.0088971],\n",
       "       [1.0089209, 1.0089209, 1.0089209, ..., 1.0089209, 1.0089209,\n",
       "        1.0089209]], dtype=float32)"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correctly_shaped_prediction_results = np.repeat(prediction_results, 11, axis=-1)\n",
    "correctly_shaped_prediction_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_transform_results = data_scaler.inverse_transform(correctly_shaped_prediction_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_results = inverse_transform_results[:, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_testing_labels = []\n",
    "# # for label in binary_results:\n",
    "# #     if label >= 0.5:\n",
    "# #         predicted_testing_labels.append(1)\n",
    "# #     else:\n",
    "# #         predicted_testing_labels.append(0)\n",
    "# for label in binary_results:\n",
    "#     predicted_testing_labels.append(label )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_testing_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_testing_labels = list(testing_data['up'])\n",
    "# actual_testing_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9995793016407236\n"
     ]
    }
   ],
   "source": [
    "correct_count = 0\n",
    "ind = 0\n",
    "correct_x = []\n",
    "correct_y = []\n",
    "wrong_x = []\n",
    "wrong_y = []\n",
    "for predicted_label, actual_label in zip(binary_results, actual_testing_labels):\n",
    "    if predicted_label >= 0.5 and actual_label == 1:\n",
    "        correct_count += 1\n",
    "        correct_x.append(ind)\n",
    "        correct_y.append(predicted_label)\n",
    "    elif predicted_label < 0.5 and actual_label == 0:\n",
    "        correct_count += 1\n",
    "        correct_x.append(ind)\n",
    "        correct_y.append(predicted_label)\n",
    "    elif predicted_label >= 0.5 and actual_label == 0:\n",
    "        wrong_x.append(ind)\n",
    "        wrong_y.append(predicted_label)\n",
    "    elif predicted_label < 0.5 and actual_label == 1:\n",
    "        wrong_x.append(ind)\n",
    "        wrong_y.append(predicted_label)\n",
    "    ind+=1\n",
    "\n",
    "# print out ratio of correct predictions\n",
    "print(correct_count/len(actual_testing_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the correct and incorrect predictions\n",
    "plt.scatter(correct_x, correct_y, c='green')\n",
    "plt.scatter(wrong_x, wrong_y, c='red')\n",
    "# plt.scatter([x for x in range(len(actual_testing_labels))], actual_testing_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BELOW USED FOR TESTING. RUN OPTIONALLY (takes excessively long time [4-5 hours])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing different values of time step for accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_52 (LSTM)               (None, 1, 64)             19456     \n",
      "_________________________________________________________________\n",
      "lstm_53 (LSTM)               (None, 96)                61824     \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 81,377\n",
      "Trainable params: 81,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "5471/5471 [==============================] - 21s 3ms/step - loss: 1.0370 - val_loss: 0.1535\n",
      "Epoch 2/3\n",
      "5471/5471 [==============================] - 16s 3ms/step - loss: 0.1341 - val_loss: 0.0310\n",
      "Epoch 3/3\n",
      "5471/5471 [==============================] - 16s 3ms/step - loss: 0.0803 - val_loss: 0.0297\n",
      "2377\n",
      "2378\n",
      "0.9995794785534062\n",
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_54 (LSTM)               (None, 2, 64)             19456     \n",
      "_________________________________________________________________\n",
      "lstm_55 (LSTM)               (None, 96)                61824     \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 81,377\n",
      "Trainable params: 81,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "5470/5470 [==============================] - 25s 4ms/step - loss: 0.9622 - val_loss: 0.0528\n",
      "Epoch 2/3\n",
      "5470/5470 [==============================] - 20s 4ms/step - loss: 0.0977 - val_loss: 0.0379\n",
      "Epoch 3/3\n",
      "5470/5470 [==============================] - 20s 4ms/step - loss: 0.0655 - val_loss: 0.0340\n",
      "2360\n",
      "2378\n",
      "0.992430613961312\n",
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_56 (LSTM)               (None, 3, 64)             19456     \n",
      "_________________________________________________________________\n",
      "lstm_57 (LSTM)               (None, 96)                61824     \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 81,377\n",
      "Trainable params: 81,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "5469/5469 [==============================] - 30s 5ms/step - loss: 0.9124 - val_loss: 0.0776\n",
      "Epoch 2/3\n",
      "5469/5469 [==============================] - 25s 5ms/step - loss: 0.0921 - val_loss: 0.0322\n",
      "Epoch 3/3\n",
      "5469/5469 [==============================] - 24s 4ms/step - loss: 0.0680 - val_loss: 0.0374\n",
      "2343\n",
      "2378\n",
      "0.9852817493692179\n",
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_58 (LSTM)               (None, 4, 64)             19456     \n",
      "_________________________________________________________________\n",
      "lstm_59 (LSTM)               (None, 96)                61824     \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 81,377\n",
      "Trainable params: 81,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "5468/5468 [==============================] - 32s 5ms/step - loss: 0.8220 - val_loss: 0.0347\n",
      "Epoch 2/3\n",
      "5468/5468 [==============================] - 29s 5ms/step - loss: 0.0864 - val_loss: 0.0323\n",
      "Epoch 3/3\n",
      "5468/5468 [==============================] - 28s 5ms/step - loss: 0.0640 - val_loss: 0.0339\n",
      "2326\n",
      "2378\n",
      "0.9781328847771237\n",
      "Model: \"sequential_30\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_60 (LSTM)               (None, 5, 64)             19456     \n",
      "_________________________________________________________________\n",
      "lstm_61 (LSTM)               (None, 96)                61824     \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 81,377\n",
      "Trainable params: 81,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "5467/5467 [==============================] - 37s 6ms/step - loss: 0.8315 - val_loss: 0.0815\n",
      "Epoch 2/3\n",
      "5467/5467 [==============================] - 32s 6ms/step - loss: 0.0792 - val_loss: 0.0328\n",
      "Epoch 3/3\n",
      "5467/5467 [==============================] - 32s 6ms/step - loss: 0.0602 - val_loss: 0.0330\n",
      "2309\n",
      "2378\n",
      "0.9709840201850294\n",
      "Model: \"sequential_31\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_62 (LSTM)               (None, 6, 64)             19456     \n",
      "_________________________________________________________________\n",
      "lstm_63 (LSTM)               (None, 96)                61824     \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 81,377\n",
      "Trainable params: 81,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "5466/5466 [==============================] - 40s 7ms/step - loss: 0.8377 - val_loss: 0.0385\n",
      "Epoch 2/3\n",
      "5466/5466 [==============================] - 35s 6ms/step - loss: 0.0730 - val_loss: 0.0344\n",
      "Epoch 3/3\n",
      "5466/5466 [==============================] - 35s 6ms/step - loss: 0.0636 - val_loss: 0.0335\n",
      "2292\n",
      "2378\n",
      "0.9638351555929352\n",
      "Model: \"sequential_32\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_64 (LSTM)               (None, 7, 64)             19456     \n",
      "_________________________________________________________________\n",
      "lstm_65 (LSTM)               (None, 96)                61824     \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 81,377\n",
      "Trainable params: 81,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5465/5465 [==============================] - 41s 7ms/step - loss: 0.8414 - val_loss: 0.0537\n",
      "Epoch 2/3\n",
      "5465/5465 [==============================] - 38s 7ms/step - loss: 0.0791 - val_loss: 0.0348\n",
      "Epoch 3/3\n",
      "5465/5465 [==============================] - 40s 7ms/step - loss: 0.0592 - val_loss: 0.0355\n",
      "2275\n",
      "2378\n",
      "0.956686291000841\n",
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_66 (LSTM)               (None, 8, 64)             19456     \n",
      "_________________________________________________________________\n",
      "lstm_67 (LSTM)               (None, 96)                61824     \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 81,377\n",
      "Trainable params: 81,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "5464/5464 [==============================] - 49s 8ms/step - loss: 0.7693 - val_loss: 0.0366\n",
      "Epoch 2/3\n",
      "5464/5464 [==============================] - 43s 8ms/step - loss: 0.0784 - val_loss: 0.0346\n",
      "Epoch 3/3\n",
      "5464/5464 [==============================] - 42s 8ms/step - loss: 0.0669 - val_loss: 0.0338\n",
      "2258\n",
      "2378\n",
      "0.9495374264087468\n",
      "Model: \"sequential_34\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_68 (LSTM)               (None, 9, 64)             19456     \n",
      "_________________________________________________________________\n",
      "lstm_69 (LSTM)               (None, 96)                61824     \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 81,377\n",
      "Trainable params: 81,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "5463/5463 [==============================] - 51s 9ms/step - loss: 0.8179 - val_loss: 0.0404\n",
      "Epoch 2/3\n",
      "5463/5463 [==============================] - 48s 9ms/step - loss: 0.0851 - val_loss: 0.0393\n",
      "Epoch 3/3\n",
      "5463/5463 [==============================] - 44s 8ms/step - loss: 0.0629 - val_loss: 0.0380\n",
      "2241\n",
      "2378\n",
      "0.9423885618166526\n",
      "Model: \"sequential_35\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_70 (LSTM)               (None, 10, 64)            19456     \n",
      "_________________________________________________________________\n",
      "lstm_71 (LSTM)               (None, 96)                61824     \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 81,377\n",
      "Trainable params: 81,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "5462/5462 [==============================] - 52s 9ms/step - loss: 0.7695 - val_loss: 0.0410\n",
      "Epoch 2/3\n",
      "5462/5462 [==============================] - 52s 9ms/step - loss: 0.0863 - val_loss: 0.0380\n",
      "Epoch 3/3\n",
      "5462/5462 [==============================] - 54s 10ms/step - loss: 0.0667 - val_loss: 0.0434\n",
      "2224\n",
      "2378\n",
      "0.9352396972245585\n",
      "Model: \"sequential_36\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_72 (LSTM)               (None, 11, 64)            19456     \n",
      "_________________________________________________________________\n",
      "lstm_73 (LSTM)               (None, 96)                61824     \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 81,377\n",
      "Trainable params: 81,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "5461/5461 [==============================] - 67s 12ms/step - loss: 0.7641 - val_loss: 0.0643\n",
      "Epoch 2/3\n",
      "5461/5461 [==============================] - 51s 9ms/step - loss: 0.0790 - val_loss: 0.0468\n",
      "Epoch 3/3\n",
      "5461/5461 [==============================] - 53s 10ms/step - loss: 0.0610 - val_loss: 0.0358\n",
      "2208\n",
      "2378\n",
      "0.928511354079058\n",
      "Model: \"sequential_37\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_74 (LSTM)               (None, 12, 64)            19456     \n",
      "_________________________________________________________________\n",
      "lstm_75 (LSTM)               (None, 96)                61824     \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 81,377\n",
      "Trainable params: 81,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "5460/5460 [==============================] - 62s 10ms/step - loss: 0.7374 - val_loss: 0.0476\n",
      "Epoch 2/3\n",
      "5460/5460 [==============================] - 54s 10ms/step - loss: 0.0738 - val_loss: 0.0401\n",
      "Epoch 3/3\n",
      "5460/5460 [==============================] - 56s 10ms/step - loss: 0.0551 - val_loss: 0.0363\n",
      "2196\n",
      "2378\n",
      "0.9234650967199327\n",
      "Model: \"sequential_38\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_76 (LSTM)               (None, 13, 64)            19456     \n",
      "_________________________________________________________________\n",
      "lstm_77 (LSTM)               (None, 96)                61824     \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 81,377\n",
      "Trainable params: 81,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "5459/5459 [==============================] - 67s 12ms/step - loss: 0.7698 - val_loss: 0.0488\n",
      "Epoch 2/3\n",
      "5459/5459 [==============================] - 63s 12ms/step - loss: 0.0798 - val_loss: 0.0580\n",
      "Epoch 3/3\n",
      "5459/5459 [==============================] - 57s 10ms/step - loss: 0.0629 - val_loss: 0.0364\n",
      "2173\n",
      "2378\n",
      "0.9137931034482759\n",
      "Model: \"sequential_39\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_78 (LSTM)               (None, 14, 64)            19456     \n",
      "_________________________________________________________________\n",
      "lstm_79 (LSTM)               (None, 96)                61824     \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 81,377\n",
      "Trainable params: 81,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "5458/5458 [==============================] - 63s 11ms/step - loss: 0.7503 - val_loss: 0.0559\n",
      "Epoch 2/3\n",
      "5458/5458 [==============================] - 71s 13ms/step - loss: 0.0769 - val_loss: 0.0387\n",
      "Epoch 3/3\n",
      "5458/5458 [==============================] - 69s 13ms/step - loss: 0.0710 - val_loss: 0.0389\n",
      "2156\n",
      "2378\n",
      "0.9066442388561816\n",
      "Model: \"sequential_40\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_80 (LSTM)               (None, 15, 64)            19456     \n",
      "_________________________________________________________________\n",
      "lstm_81 (LSTM)               (None, 96)                61824     \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 81,377\n",
      "Trainable params: 81,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "5457/5457 [==============================] - 73s 13ms/step - loss: 0.7511 - val_loss: 0.0391\n",
      "Epoch 2/3\n",
      "5457/5457 [==============================] - 68s 12ms/step - loss: 0.0786 - val_loss: 0.0854\n",
      "Epoch 3/3\n",
      "5457/5457 [==============================] - 94s 17ms/step - loss: 0.0676 - val_loss: 0.0366\n",
      "2139\n",
      "2378\n",
      "0.8994953742640874\n",
      "Model: \"sequential_41\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_82 (LSTM)               (None, 16, 64)            19456     \n",
      "_________________________________________________________________\n",
      "lstm_83 (LSTM)               (None, 96)                61824     \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 81,377\n",
      "Trainable params: 81,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "5456/5456 [==============================] - 102s 18ms/step - loss: 0.7079 - val_loss: 0.0472\n",
      "Epoch 2/3\n",
      "5456/5456 [==============================] - 100s 18ms/step - loss: 0.0766 - val_loss: 0.0568\n",
      "Epoch 3/3\n",
      "5456/5456 [==============================] - 90s 17ms/step - loss: 0.0688 - val_loss: 0.0355\n",
      "2122\n",
      "2378\n",
      "0.8923465096719932\n",
      "Model: \"sequential_42\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_84 (LSTM)               (None, 17, 64)            19456     \n",
      "_________________________________________________________________\n",
      "lstm_85 (LSTM)               (None, 96)                61824     \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 81,377\n",
      "Trainable params: 81,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "5455/5455 [==============================] - 94s 15ms/step - loss: 0.6909 - val_loss: 0.0420\n",
      "Epoch 2/3\n",
      "5455/5455 [==============================] - 90s 17ms/step - loss: 0.0731 - val_loss: 0.0380\n",
      "Epoch 3/3\n",
      "5455/5455 [==============================] - 82s 15ms/step - loss: 0.0640 - val_loss: 0.0355\n",
      "2105\n",
      "2378\n",
      "0.8851976450798991\n",
      "Model: \"sequential_43\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_86 (LSTM)               (None, 18, 64)            19456     \n",
      "_________________________________________________________________\n",
      "lstm_87 (LSTM)               (None, 96)                61824     \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 81,377\n",
      "Trainable params: 81,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "5454/5454 [==============================] - 129s 23ms/step - loss: 0.7791 - val_loss: 0.0532\n",
      "Epoch 2/3\n",
      "5454/5454 [==============================] - 89s 16ms/step - loss: 0.0763 - val_loss: 0.0385\n",
      "Epoch 3/3\n",
      "5454/5454 [==============================] - 102s 19ms/step - loss: 0.0660 - val_loss: 0.0386\n",
      "2088\n",
      "2378\n",
      "0.8780487804878049\n",
      "Model: \"sequential_44\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_88 (LSTM)               (None, 19, 64)            19456     \n",
      "_________________________________________________________________\n",
      "lstm_89 (LSTM)               (None, 96)                61824     \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 81,377\n",
      "Trainable params: 81,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "5453/5453 [==============================] - 99s 17ms/step - loss: 0.7176 - val_loss: 0.0432\n",
      "Epoch 2/3\n",
      "5453/5453 [==============================] - 105s 19ms/step - loss: 0.0801 - val_loss: 0.0389\n",
      "Epoch 3/3\n",
      "5453/5453 [==============================] - 117s 22ms/step - loss: 0.0594 - val_loss: 0.0375\n",
      "2071\n",
      "2378\n",
      "0.8708999158957107\n"
     ]
    }
   ],
   "source": [
    "window_accuracies = []\n",
    "for window in range (1,20):\n",
    "    training_ts_generator = TimeseriesGenerator(\n",
    "        data=normalized_training_data,\n",
    "        targets=normalized_training_labels,\n",
    "    #     targets=training_labels,\n",
    "        length=window,\n",
    "        batch_size=1\n",
    "    )\n",
    "\n",
    "    testing_ts_generator = TimeseriesGenerator(\n",
    "        data=normalized_testing_data,\n",
    "        targets=normalized_testing_labels,\n",
    "    #     targets=testing_labels,\n",
    "        length=window,\n",
    "        batch_size=1\n",
    "    ) \n",
    "    training_model = Sequential()\n",
    "    # input_shape for each data input is window_size x number of features (column) in training_data\n",
    "    training_model.add(LSTM(64, activation='sigmoid', input_shape=(window, 11), return_sequences=True))\n",
    "    training_model.add(LSTM(96, activation='sigmoid', return_sequences=False))\n",
    "    training_model.add(Dropout(0.2))\n",
    "\n",
    "    # output either standardized 0 or 1\n",
    "    training_model.add(Dense(1))\n",
    "\n",
    "    training_model.compile(optimizer='SGD', loss='mse')\n",
    "    training_model.summary()\n",
    "    \n",
    "    history_train = training_model.fit(training_ts_generator, validation_data=testing_ts_generator, epochs=3)\n",
    "    \n",
    "#     training_results = training_model.evaluate(training_ts_generator)\n",
    "#     testing_results = training_model.evaluate(testing_ts_generator)\n",
    "    \n",
    "    prediction_results = training_model.predict(testing_ts_generator)\n",
    "    \n",
    "    correctly_shaped_prediction_results = np.repeat(prediction_results, 11, axis=-1)\n",
    "\n",
    "    inverse_transform_results = data_scaler.inverse_transform(correctly_shaped_prediction_results)\n",
    "    \n",
    "    binary_results = inverse_transform_results[:, 10]\n",
    "    \n",
    "    predicted_testing_labels = []\n",
    "    for label in binary_results:\n",
    "        if label >= 0.5:\n",
    "            predicted_testing_labels.append(1)\n",
    "        else:\n",
    "            predicted_testing_labels.append(0)\n",
    "            \n",
    "    correct_count = 0\n",
    "    for predicted_label, actual_label in zip(predicted_testing_labels, actual_testing_labels):\n",
    "        if predicted_label == actual_label:\n",
    "            correct_count += 1\n",
    "    print(correct_count)\n",
    "    print(len(actual_testing_labels))\n",
    "\n",
    "    # print out correct ratio\n",
    "    current_accuracy = correct_count/len(actual_testing_labels)\n",
    "    print(current_accuracy)\n",
    "    window_accuracies.append(current_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the sigmoid accuracies\n",
    "window_accuracies = [\n",
    "    0.9995794785534062,\n",
    "    0.992430613961312,\n",
    "    0.9852817493692179,\n",
    "    0.9781328847771237,\n",
    "    0.9709840201850294,\n",
    "    0.9638351555929352,\n",
    "    0.956686291000841,\n",
    "    0.9495374264087468,\n",
    "    0.9423885618166526,\n",
    "    0.9352396972245585,\n",
    "    0.928511354079058,\n",
    "    0.9234650967199327,\n",
    "    0.9137931034482759,\n",
    "    0.9066442388561816,\n",
    "    0.8994953742640874,\n",
    "    0.8923465096719932,\n",
    "    0.8851976450798991,\n",
    "    0.8780487804878049,\n",
    "    0.8708999158957107\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([i for i in range(1, 20)], window_accuracies)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot, it appears that the more lagged observations are given, the lower the accuracy becomes. Although there is a shift in trend, based on the amount of time it takes to run the network (about 8 minutes per network with 5 epochs), this will be an area for further improvment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we extend to 72 for the time steps size, the accuracy goes all the way to approximately 50%. Two such networks are saved in the keras_models folder. One represents a network that uses the sentiment data, while the other does not use the sentiment data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_10 (LSTM)               (None, 1, 64)             19456     \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 96)                61824     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 81,377\n",
      "Trainable params: 81,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "5471/5471 [==============================] - 22s 3ms/step - loss: 0.6184 - val_loss: 0.0410\n",
      "Epoch 2/3\n",
      "5471/5471 [==============================] - 18s 3ms/step - loss: 0.0339 - val_loss: 0.0281\n",
      "Epoch 3/3\n",
      "5471/5471 [==============================] - 15s 3ms/step - loss: 0.0377 - val_loss: 0.0287\n",
      "2376\n",
      "2377\n",
      "0.9995793016407236\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_12 (LSTM)               (None, 2, 64)             19456     \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 96)                61824     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 81,377\n",
      "Trainable params: 81,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "5470/5470 [==============================] - 25s 4ms/step - loss: 0.3774 - val_loss: 0.0406\n",
      "Epoch 2/3\n",
      "5470/5470 [==============================] - 20s 4ms/step - loss: 0.0506 - val_loss: 0.0364\n",
      "Epoch 3/3\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 0.0458 - val_loss: 0.0361\n",
      "2375\n",
      "2377\n",
      "0.9991586032814472\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_14 (LSTM)               (None, 3, 64)             19456     \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, 96)                61824     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 81,377\n",
      "Trainable params: 81,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "5469/5469 [==============================] - 28s 4ms/step - loss: 0.2564 - val_loss: 0.0466\n",
      "Epoch 2/3\n",
      "5469/5469 [==============================] - 23s 4ms/step - loss: 0.0677 - val_loss: 0.0536\n",
      "Epoch 3/3\n",
      "5469/5469 [==============================] - 21s 4ms/step - loss: 0.0546 - val_loss: 0.0427\n",
      "2358\n",
      "2377\n",
      "0.9920067311737484\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_16 (LSTM)               (None, 4, 64)             19456     \n",
      "_________________________________________________________________\n",
      "lstm_17 (LSTM)               (None, 96)                61824     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 81,377\n",
      "Trainable params: 81,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "5468/5468 [==============================] - 29s 5ms/step - loss: 0.2242 - val_loss: 0.0569\n",
      "Epoch 2/3\n",
      "5468/5468 [==============================] - 25s 5ms/step - loss: 0.0647 - val_loss: 0.0524\n",
      "Epoch 3/3\n",
      "5468/5468 [==============================] - 33s 6ms/step - loss: 0.0699 - val_loss: 0.0471\n",
      "2341\n",
      "2377\n",
      "0.9848548590660496\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_18 (LSTM)               (None, 5, 64)             19456     \n",
      "_________________________________________________________________\n",
      "lstm_19 (LSTM)               (None, 96)                61824     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 81,377\n",
      "Trainable params: 81,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "5467/5467 [==============================] - 36s 6ms/step - loss: 0.2396 - val_loss: 0.0694\n",
      "Epoch 2/3\n",
      "5467/5467 [==============================] - 31s 6ms/step - loss: 0.0680 - val_loss: 0.0504\n",
      "Epoch 3/3\n",
      "5467/5467 [==============================] - 34s 6ms/step - loss: 0.0597 - val_loss: 0.0507\n",
      "2324\n",
      "2377\n",
      "0.9777029869583509\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_20 (LSTM)               (None, 6, 64)             19456     \n",
      "_________________________________________________________________\n",
      "lstm_21 (LSTM)               (None, 96)                61824     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 81,377\n",
      "Trainable params: 81,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "5466/5466 [==============================] - 47s 8ms/step - loss: 0.1997 - val_loss: 0.0640\n",
      "Epoch 2/3\n",
      "5466/5466 [==============================] - 35s 6ms/step - loss: 0.0666 - val_loss: 0.0719\n",
      "Epoch 3/3\n",
      "5466/5466 [==============================] - 46s 8ms/step - loss: 0.0671 - val_loss: 0.0455\n",
      "2299\n",
      "2377\n",
      "0.9671855279764409\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_22 (LSTM)               (None, 7, 64)             19456     \n",
      "_________________________________________________________________\n",
      "lstm_23 (LSTM)               (None, 96)                61824     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 81,377\n",
      "Trainable params: 81,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5465/5465 [==============================] - 64s 11ms/step - loss: 0.2033 - val_loss: 0.0880\n",
      "Epoch 2/3\n",
      "5465/5465 [==============================] - 47s 9ms/step - loss: 0.0769 - val_loss: 0.0669\n",
      "Epoch 3/3\n",
      "5465/5465 [==============================] - 43s 8ms/step - loss: 0.0599 - val_loss: 0.0340\n",
      "2274\n",
      "2377\n",
      "0.9566680689945309\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_24 (LSTM)               (None, 8, 64)             19456     \n",
      "_________________________________________________________________\n",
      "lstm_25 (LSTM)               (None, 96)                61824     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 81,377\n",
      "Trainable params: 81,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "5464/5464 [==============================] - 54s 9ms/step - loss: 0.1997 - val_loss: 0.0721\n",
      "Epoch 2/3\n",
      "5464/5464 [==============================] - 44s 8ms/step - loss: 0.0666 - val_loss: 0.0794\n",
      "Epoch 3/3\n",
      "5464/5464 [==============================] - 44s 8ms/step - loss: 0.0591 - val_loss: 0.0366\n",
      "2265\n",
      "2377\n",
      "0.9528817837610434\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_26 (LSTM)               (None, 9, 64)             19456     \n",
      "_________________________________________________________________\n",
      "lstm_27 (LSTM)               (None, 96)                61824     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 81,377\n",
      "Trainable params: 81,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "5463/5463 [==============================] - 55s 9ms/step - loss: 0.1889 - val_loss: 0.0546\n",
      "Epoch 2/3\n",
      "5463/5463 [==============================] - 50s 9ms/step - loss: 0.0623 - val_loss: 0.0362\n",
      "Epoch 3/3\n",
      "5463/5463 [==============================] - 49s 9ms/step - loss: 0.0545 - val_loss: 0.0309\n",
      "2240\n",
      "2377\n",
      "0.9423643247791333\n"
     ]
    }
   ],
   "source": [
    "window_accuracies_relu = []\n",
    "for window in range (1,10):\n",
    "    training_ts_generator = TimeseriesGenerator(\n",
    "        data=normalized_training_data,\n",
    "        targets=normalized_training_labels,\n",
    "    #     targets=training_labels,\n",
    "        length=window,\n",
    "        batch_size=1\n",
    "    )\n",
    "\n",
    "    testing_ts_generator = TimeseriesGenerator(\n",
    "        data=normalized_testing_data,\n",
    "        targets=normalized_testing_labels,\n",
    "    #     targets=testing_labels,\n",
    "        length=window,\n",
    "        batch_size=1\n",
    "    ) \n",
    "    training_model = Sequential()\n",
    "    # input_shape for each data input is window_size x number of features (column) in training_data\n",
    "    training_model.add(LSTM(64, activation='relu', input_shape=(window, 11), return_sequences=True))\n",
    "    training_model.add(LSTM(96, activation='relu', return_sequences=False))\n",
    "    training_model.add(Dropout(0.2))\n",
    "\n",
    "    # output either standardized 0 or 1\n",
    "    training_model.add(Dense(1))\n",
    "\n",
    "    training_model.compile(optimizer='SGD', loss='mse')\n",
    "    training_model.summary()\n",
    "    \n",
    "    history_train = training_model.fit(training_ts_generator, validation_data=testing_ts_generator, epochs=3)\n",
    "    \n",
    "#     training_results = training_model.evaluate(training_ts_generator)\n",
    "#     testing_results = training_model.evaluate(testing_ts_generator)\n",
    "    \n",
    "    prediction_results = training_model.predict(testing_ts_generator)\n",
    "    \n",
    "    correctly_shaped_prediction_results = np.repeat(prediction_results, 11, axis=-1)\n",
    "\n",
    "    inverse_transform_results = data_scaler.inverse_transform(correctly_shaped_prediction_results)\n",
    "    \n",
    "    binary_results = inverse_transform_results[:, 10]\n",
    "    \n",
    "    predicted_testing_labels = []\n",
    "    for label in binary_results:\n",
    "        if label >= 0.5:\n",
    "            predicted_testing_labels.append(1)\n",
    "        else:\n",
    "            predicted_testing_labels.append(0)\n",
    "            \n",
    "    correct_count = 0\n",
    "    for predicted_label, actual_label in zip(predicted_testing_labels, actual_testing_labels):\n",
    "        if predicted_label == actual_label:\n",
    "            correct_count += 1\n",
    "    print(correct_count)\n",
    "    print(len(actual_testing_labels))\n",
    "\n",
    "    # print out correct ratio\n",
    "    current_accuracy = correct_count/len(actual_testing_labels)\n",
    "    print(current_accuracy)\n",
    "    window_accuracies_relu.append(current_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_28 (LSTM)               (None, 1, 64)             19456     \n",
      "_________________________________________________________________\n",
      "lstm_29 (LSTM)               (None, 96)                61824     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 81,377\n",
      "Trainable params: 81,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "5471/5471 [==============================] - 23s 4ms/step - loss: 1.0113 - val_loss: 0.9736\n",
      "Epoch 2/3\n",
      "5471/5471 [==============================] - 18s 3ms/step - loss: 1.0088 - val_loss: 0.9997\n",
      "Epoch 3/3\n",
      "5471/5471 [==============================] - 19s 3ms/step - loss: 1.0110 - val_loss: 1.0343\n",
      "936\n",
      "2377\n",
      "0.3937736642827093\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_30 (LSTM)               (None, 2, 64)             19456     \n",
      "_________________________________________________________________\n",
      "lstm_31 (LSTM)               (None, 96)                61824     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 81,377\n",
      "Trainable params: 81,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "5470/5470 [==============================] - 28s 4ms/step - loss: 1.0105 - val_loss: 1.1112\n",
      "Epoch 2/3\n",
      "5470/5470 [==============================] - 22s 4ms/step - loss: 1.0125 - val_loss: 1.0009\n",
      "Epoch 3/3\n",
      "5470/5470 [==============================] - 22s 4ms/step - loss: 1.0092 - val_loss: 1.0897\n",
      "935\n",
      "2377\n",
      "0.3933529659234329\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_32 (LSTM)               (None, 3, 64)             19456     \n",
      "_________________________________________________________________\n",
      "lstm_33 (LSTM)               (None, 96)                61824     \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 81,377\n",
      "Trainable params: 81,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "5469/5469 [==============================] - 35s 6ms/step - loss: 1.0104 - val_loss: 0.9999\n",
      "Epoch 2/3\n",
      "5469/5469 [==============================] - 26s 5ms/step - loss: 1.0123 - val_loss: 0.9673\n",
      "Epoch 3/3\n",
      "5469/5469 [==============================] - 25s 5ms/step - loss: 1.0145 - val_loss: 1.0028\n",
      "934\n",
      "2377\n",
      "0.3929322675641565\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_34 (LSTM)               (None, 4, 64)             19456     \n",
      "_________________________________________________________________\n",
      "lstm_35 (LSTM)               (None, 96)                61824     \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 81,377\n",
      "Trainable params: 81,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "5468/5468 [==============================] - 35s 6ms/step - loss: 1.0089 - val_loss: 0.9551\n",
      "Epoch 2/3\n",
      "5468/5468 [==============================] - 31s 6ms/step - loss: 1.0157 - val_loss: 0.9864\n",
      "Epoch 3/3\n",
      "5468/5468 [==============================] - 33s 6ms/step - loss: 1.0098 - val_loss: 1.0905\n",
      "933\n",
      "2377\n",
      "0.3925115692048801\n",
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_36 (LSTM)               (None, 5, 64)             19456     \n",
      "_________________________________________________________________\n",
      "lstm_37 (LSTM)               (None, 96)                61824     \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 81,377\n",
      "Trainable params: 81,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "5467/5467 [==============================] - 41s 7ms/step - loss: 1.0099 - val_loss: 1.0621\n",
      "Epoch 2/3\n",
      "5467/5467 [==============================] - 40s 7ms/step - loss: 1.0128 - val_loss: 0.9796\n",
      "Epoch 3/3\n",
      "5467/5467 [==============================] - 35s 6ms/step - loss: 1.0093 - val_loss: 1.0023\n",
      "932\n",
      "2377\n",
      "0.3920908708456037\n",
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_38 (LSTM)               (None, 6, 64)             19456     \n",
      "_________________________________________________________________\n",
      "lstm_39 (LSTM)               (None, 96)                61824     \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 81,377\n",
      "Trainable params: 81,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "5466/5466 [==============================] - 49s 8ms/step - loss: 1.0114 - val_loss: 0.9680\n",
      "Epoch 2/3\n",
      "5466/5466 [==============================] - 44s 8ms/step - loss: 1.0133 - val_loss: 0.9899\n",
      "Epoch 3/3\n",
      "5466/5466 [==============================] - 42s 8ms/step - loss: 1.0078 - val_loss: 0.9646\n",
      "1440\n",
      "2377\n",
      "0.6058056373580143\n",
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_40 (LSTM)               (None, 7, 64)             19456     \n",
      "_________________________________________________________________\n",
      "lstm_41 (LSTM)               (None, 96)                61824     \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 81,377\n",
      "Trainable params: 81,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5465/5465 [==============================] - 58s 9ms/step - loss: 1.0121 - val_loss: 0.9914\n",
      "Epoch 2/3\n",
      "5465/5465 [==============================] - 41s 7ms/step - loss: 1.0130 - val_loss: 0.9710\n",
      "Epoch 3/3\n",
      "5465/5465 [==============================] - 44s 8ms/step - loss: 1.0065 - val_loss: 0.9712\n",
      "1440\n",
      "2377\n",
      "0.6058056373580143\n",
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_42 (LSTM)               (None, 8, 64)             19456     \n",
      "_________________________________________________________________\n",
      "lstm_43 (LSTM)               (None, 96)                61824     \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 81,377\n",
      "Trainable params: 81,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "5464/5464 [==============================] - 50s 9ms/step - loss: 1.0090 - val_loss: 0.9804\n",
      "Epoch 2/3\n",
      "5464/5464 [==============================] - 45s 8ms/step - loss: 1.0114 - val_loss: 1.0253\n",
      "Epoch 3/3\n",
      "5464/5464 [==============================] - 52s 10ms/step - loss: 1.0086 - val_loss: 1.0544\n",
      "929\n",
      "2377\n",
      "0.39082877576777453\n",
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_44 (LSTM)               (None, 9, 64)             19456     \n",
      "_________________________________________________________________\n",
      "lstm_45 (LSTM)               (None, 96)                61824     \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 81,377\n",
      "Trainable params: 81,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "5463/5463 [==============================] - 57s 10ms/step - loss: 1.0100 - val_loss: 0.9675\n",
      "Epoch 2/3\n",
      "5463/5463 [==============================] - 51s 9ms/step - loss: 1.0082 - val_loss: 0.9776\n",
      "Epoch 3/3\n",
      "5463/5463 [==============================] - 51s 9ms/step - loss: 1.0126 - val_loss: 1.0123\n",
      "928\n",
      "2377\n",
      "0.3904080774084981\n"
     ]
    }
   ],
   "source": [
    "window_accuracies_softmax = []\n",
    "for window in range (1,10):\n",
    "    training_ts_generator = TimeseriesGenerator(\n",
    "        data=normalized_training_data,\n",
    "        targets=normalized_training_labels,\n",
    "    #     targets=training_labels,\n",
    "        length=window,\n",
    "        batch_size=1\n",
    "    )\n",
    "\n",
    "    testing_ts_generator = TimeseriesGenerator(\n",
    "        data=normalized_testing_data,\n",
    "        targets=normalized_testing_labels,\n",
    "    #     targets=testing_labels,\n",
    "        length=window,\n",
    "        batch_size=1\n",
    "    ) \n",
    "    training_model = Sequential()\n",
    "    # input_shape for each data input is window_size x number of features (column) in training_data\n",
    "    training_model.add(LSTM(64, activation='softmax', input_shape=(window, 11), return_sequences=True))\n",
    "    training_model.add(LSTM(96, activation='softmax', return_sequences=False))\n",
    "    training_model.add(Dropout(0.2))\n",
    "\n",
    "    # output either standardized 0 or 1\n",
    "    training_model.add(Dense(1))\n",
    "\n",
    "    training_model.compile(optimizer='SGD', loss='mse')\n",
    "    training_model.summary()\n",
    "    \n",
    "    history_train = training_model.fit(training_ts_generator, validation_data=testing_ts_generator, epochs=3)\n",
    "    \n",
    "#     training_results = training_model.evaluate(training_ts_generator)\n",
    "#     testing_results = training_model.evaluate(testing_ts_generator)\n",
    "    \n",
    "    prediction_results = training_model.predict(testing_ts_generator)\n",
    "    \n",
    "    correctly_shaped_prediction_results = np.repeat(prediction_results, 11, axis=-1)\n",
    "\n",
    "    inverse_transform_results = data_scaler.inverse_transform(correctly_shaped_prediction_results)\n",
    "    \n",
    "    binary_results = inverse_transform_results[:, 10]\n",
    "    \n",
    "    predicted_testing_labels = []\n",
    "    for label in binary_results:\n",
    "        if label >= 0.5:\n",
    "            predicted_testing_labels.append(1)\n",
    "        else:\n",
    "            predicted_testing_labels.append(0)\n",
    "            \n",
    "    correct_count = 0\n",
    "    for predicted_label, actual_label in zip(predicted_testing_labels, actual_testing_labels):\n",
    "        if predicted_label == actual_label:\n",
    "            correct_count += 1\n",
    "    print(correct_count)\n",
    "    print(len(actual_testing_labels))\n",
    "\n",
    "    # print out correct ratio\n",
    "    current_accuracy = correct_count/len(actual_testing_labels)\n",
    "    print(current_accuracy)\n",
    "    window_accuracies_softmax.append(current_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9995794785534062,\n",
       " 0.992430613961312,\n",
       " 0.9852817493692179,\n",
       " 0.9781328847771237,\n",
       " 0.9709840201850294,\n",
       " 0.9638351555929352,\n",
       " 0.956686291000841,\n",
       " 0.9495374264087468,\n",
       " 0.9423885618166526,\n",
       " 0.9352396972245585,\n",
       " 0.928511354079058,\n",
       " 0.9234650967199327,\n",
       " 0.9137931034482759,\n",
       " 0.9066442388561816,\n",
       " 0.8994953742640874,\n",
       " 0.8923465096719932,\n",
       " 0.8851976450798991,\n",
       " 0.8780487804878049,\n",
       " 0.8708999158957107]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([i for i in range(1, 10)], window_accuracies[:9], c='blue')\n",
    "plt.plot([i for i in range(1, 10)], window_accuracies_relu, c='orange')\n",
    "plt.plot([i for i in range(1, 10)], window_accuracies_softmax, c='green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9995794785534062,\n",
       " 0.992430613961312,\n",
       " 0.9852817493692179,\n",
       " 0.9781328847771237,\n",
       " 0.9709840201850294,\n",
       " 0.9638351555929352,\n",
       " 0.956686291000841,\n",
       " 0.9495374264087468,\n",
       " 0.9423885618166526,\n",
       " 0.9352396972245585,\n",
       " 0.928511354079058,\n",
       " 0.9234650967199327,\n",
       " 0.9137931034482759,\n",
       " 0.9066442388561816,\n",
       " 0.8994953742640874,\n",
       " 0.8923465096719932,\n",
       " 0.8851976450798991,\n",
       " 0.8780487804878049,\n",
       " 0.8708999158957107]"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save the model so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./keras_models/lstm_72_sequence_20_epoch/assets\n"
     ]
    }
   ],
   "source": [
    "# training_model.save('./keras_models/lstm_72_sequence_20_epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32, 64, 96, 128]"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_funcs = ['relu', 'sigmoid', 'softmax']\n",
    "node_values = [val for val in range(32, 129, 32)]\n",
    "drop_values = [0.2, 0.3, 0.4, 0.5]\n",
    "node_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# history_train = training_model.fit(training_ts_generator, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_results = training_model.evaluate(training_ts_generator)\n",
    "# testing_results = training_model.evaluate(testing_ts_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.5724188089370728,\n",
       "  0.0920354574918747,\n",
       "  0.07440289109945297,\n",
       "  0.06676856428384781,\n",
       "  0.06256063282489777,\n",
       "  0.0596199706196785,\n",
       "  0.056295670568943024,\n",
       "  0.05643153190612793,\n",
       "  0.0549185648560524,\n",
       "  0.05512009561061859]}"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training_results\n",
    "history_train.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 20s 3ms/step - loss: 0.4397\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0684: \n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0654\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0574\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0541\n",
      "5470/5470 [==============================] - 9s 1ms/step - loss: 0.0340\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0337\n",
      "relu, 32, 32, 0.2\n",
      "0.033741582185029984\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.3524\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0771\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0767\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0652\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0716\n",
      "5470/5470 [==============================] - 9s 1ms/step - loss: 0.0343\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0360\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.3342\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.1095\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 22s 4ms/step - loss: 0.0961\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0958\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0908\n",
      "5470/5470 [==============================] - 9s 1ms/step - loss: 0.0341\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0341\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 20s 3ms/step - loss: 0.4178\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.1125\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 20s 4ms/step - loss: 0.1201\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.1060\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 20s 4ms/step - loss: 0.1024\n",
      "5470/5470 [==============================] - 10s 2ms/step - loss: 0.0383\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 0.0413\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 22s 3ms/step - loss: 0.3950\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0507\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0523\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0498\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0485\n",
      "5470/5470 [==============================] - 10s 2ms/step - loss: 0.0364: 0s - loss: 0.\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 0.0375\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 22s 3ms/step - loss: 0.3506\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0651\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0655\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0563\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0645\n",
      "5470/5470 [==============================] - 9s 1ms/step - loss: 0.0354\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0359\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 20s 3ms/step - loss: 0.4081\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0660\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0682\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0633\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0706\n",
      "5470/5470 [==============================] - 9s 1ms/step - loss: 0.0340\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0354\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 20s 3ms/step - loss: 0.4789\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0899\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 0.0853\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0744\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0786\n",
      "5470/5470 [==============================] - 14s 2ms/step - loss: 0.0346\n",
      "2376/2376 [==============================] - 4s 1ms/step - loss: 0.0377\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 20s 3ms/step - loss: 0.3634: 0s\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0505\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0463\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0455: 0s - loss\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0532\n",
      "5470/5470 [==============================] - 9s 1ms/step - loss: 0.0337\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0365\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 21s 3ms/step - loss: 0.3744\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0495\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 26s 5ms/step - loss: 0.0526\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 28s 5ms/step - loss: 0.0456\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0455\n",
      "5470/5470 [==============================] - 11s 2ms/step - loss: 0.0387\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 0.0427A\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 21s 3ms/step - loss: 0.3654\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0707:\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 23s 4ms/step - loss: 0.0607: 0s \n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0532\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0608\n",
      "5470/5470 [==============================] - 9s 1ms/step - loss: 0.0355\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 0.0349\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 22s 3ms/step - loss: 0.3764\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0774\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0722\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0614\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0641\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0342\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0421\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.3379\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0502\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0529\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0390\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0491\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0380\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0362\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 29s 4ms/step - loss: 0.3650\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0506: 0s -\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0514\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.0421\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0506\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0346\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 0.0373\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 28s 4ms/step - loss: 0.3862\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0562\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0476:\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0559\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0509\n",
      "5470/5470 [==============================] - 9s 2ms/step - loss: 0.0345\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 0.0444\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 21s 3ms/step - loss: 0.3798\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0639\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0560\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0668\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.0578\n",
      "5470/5470 [==============================] - 9s 2ms/step - loss: 0.0360\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 0.0354\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 20s 3ms/step - loss: 0.3407\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0559\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0541\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0673\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0522\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0354\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0357\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.3864\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0739\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0661\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0725\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0654\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0363\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0383\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.3485\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0897\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0866\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0817\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0869\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0416\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0384\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.3875\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.1118\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 14s 3ms/step - loss: 0.1082\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.1117\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 14s 3ms/step - loss: 0.1110\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0376\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0398\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.3218\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0530\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0584\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0591\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0441\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0350\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0471\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.3130\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0605\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0492\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0498\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0554\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0347\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0367\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.3793\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 14s 3ms/step - loss: 0.0742\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0625\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0712\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0678\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0350\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0376\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 21s 3ms/step - loss: 0.4273\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0831\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0705\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0745: \n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 0.0756\n",
      "5470/5470 [==============================] - 13s 1ms/step - loss: 0.0350\n",
      "2376/2376 [==============================] - 4s 1ms/step - loss: 0.0445A: 0\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 21s 3ms/step - loss: 0.3401\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0547\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0450\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0367\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0479\n",
      "5470/5470 [==============================] - 9s 1ms/step - loss: 0.0351\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0350\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 22s 3ms/step - loss: 0.3545\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0541\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0500\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0474\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0532\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0370\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0359\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 21s 3ms/step - loss: 0.3994\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0755\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.0518\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0573\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 0.0499\n",
      "5470/5470 [==============================] - 11s 2ms/step - loss: 0.0352\n",
      "2376/2376 [==============================] - 4s 1ms/step - loss: 0.0410\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 26s 4ms/step - loss: 0.3834\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.0737\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0655\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0639\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0692\n",
      "5470/5470 [==============================] - 9s 1ms/step - loss: 0.0363\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 0.0400\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 21s 3ms/step - loss: 0.3440\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0417\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0447\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0524\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0424\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0458\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 20s 3ms/step - loss: 0.3702\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0568\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0484\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0465\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0460\n",
      "5470/5470 [==============================] - 9s 1ms/step - loss: 0.0489\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0382\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 20s 3ms/step - loss: 0.3512\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0554\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0530\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0567\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0522\n",
      "5470/5470 [==============================] - 9s 1ms/step - loss: 0.0349\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 0.0358\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 22s 3ms/step - loss: 0.4051\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0753\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0683\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0732\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0523\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0359\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0373\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.3651\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0613\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0539\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0586\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0545\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0355\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0346\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 22s 3ms/step - loss: 0.3447\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0887\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0808\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0729\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0728\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0389\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0447\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.3679\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.1000\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0904\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0843\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0866\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0391\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0497\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.3820\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.1201\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.1212\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.1090\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.1010\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0372\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0383\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.2877\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0477\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0521\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0557\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0443\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0344\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0346\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.3284\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0548\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0667\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0513\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0473\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0355A: 0s - loss\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0357\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.3693\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0676\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0705\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0640\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0608\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0369\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0349\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.3758\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0803\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0653\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0713\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0822\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0366\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0375\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.3422: 0s - los\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0531\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0456\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0588\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0428\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0344\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0373\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 20s 3ms/step - loss: 0.3244\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0509\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0502\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0496\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0495\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0343\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0354\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.3580\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0595\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0579\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0513\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0555\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0403\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0371\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 21s 3ms/step - loss: 0.3632\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0664\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0636\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0672\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0610\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0361\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0354\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.3519\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0485\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0473\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0490\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0425\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0359\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0345\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 20s 3ms/step - loss: 0.3744\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0540\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0491\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0417\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0483\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0347\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0360\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.3145\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0523: 0s - loss: 0.0\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0496\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0536\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0451\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0364\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0453\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.3645\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0658\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0669\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0515\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0644\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0358\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0352\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.3264\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0538\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0679\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0623: 0\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0532\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0357\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0355\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.3323\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0737\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0699\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0707\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0673\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0356\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0361\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.3410\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.1176\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.1053\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0960\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0924\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0407\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0405\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.3941\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.1173\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.1054\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.1144\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.1094\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0401A: 1s -  -\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0470\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.3018\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0585\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0532\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0434\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0488\n",
      "5470/5470 [==============================] - 10s 1ms/step - loss: 0.0355: 0s - loss:\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0355\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.3282\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0560\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0670\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0495\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0522\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0446\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0627\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.3440\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0777\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0595\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0731\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0552\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0350\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0361\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 20s 3ms/step - loss: 0.3545\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0801\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0777\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0749\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0762\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0360\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0359\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.3870\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0579\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0432\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0482\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0511\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0455\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0357\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.3323\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0657\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0557\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0564\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0557\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0345\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0391\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5470/5470 [==============================] - 20s 3ms/step - loss: 0.3252\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 22s 4ms/step - loss: 0.0617\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 21s 4ms/step - loss: 0.0666\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0665\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0546\n",
      "5470/5470 [==============================] - 9s 1ms/step - loss: 0.0540\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 0.0347\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 22s 3ms/step - loss: 0.3412\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.0639\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.0670\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.0726\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.0619\n",
      "5470/5470 [==============================] - 9s 2ms/step - loss: 0.0391\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 0.0524A: 0s - l\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 23s 4ms/step - loss: 0.3457\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 0.0471: 0s - lo\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 0.0475\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 0.0422\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 0.0473\n",
      "5470/5470 [==============================] - 9s 2ms/step - loss: 0.0435\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 0.0445\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 23s 3ms/step - loss: 0.3518\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.0565\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.0515\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.0439\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0529\n",
      "5470/5470 [==============================] - 9s 2ms/step - loss: 0.0350\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 0.0366\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 23s 3ms/step - loss: 0.3395\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 0.0574\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.0602\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 0.0543\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.0494\n",
      "5470/5470 [==============================] - 10s 2ms/step - loss: 0.0352: 1s - loss: 0.035 - ETA: 1s - loss: 0.\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 0.0362\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 23s 4ms/step - loss: 0.3679\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 20s 4ms/step - loss: 0.0658\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 20s 4ms/step - loss: 0.0589: 0s\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 0.0611\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 0.0590\n",
      "5470/5470 [==============================] - 10s 2ms/step - loss: 0.0354: 0s - los\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 0.0442\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 20s 3ms/step - loss: 0.8359\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0990\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0710\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0702\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0634\n",
      "5470/5470 [==============================] - 9s 1ms/step - loss: 0.0337\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0312\n",
      "sigmoid, 32, 32, 0.2\n",
      "0.03116992674767971\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 20s 3ms/step - loss: 0.9929: - ETA: 4\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.1322\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0988\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0863\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0782\n",
      "5470/5470 [==============================] - 9s 1ms/step - loss: 0.0300\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0313\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.0498\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.2148\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.1214\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.1153\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.1101\n",
      "5470/5470 [==============================] - 9s 1ms/step - loss: 0.0336\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0336\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 20s 3ms/step - loss: 1.0686\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.3020\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.1520\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.1247\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.1266\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0316\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0324\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 20s 3ms/step - loss: 0.8484\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0855\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0646\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0638\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0550\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0295\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0294\n",
      "sigmoid, 32, 64, 0.2\n",
      "0.02943655103445053\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 20s 3ms/step - loss: 0.8991\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.1252\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0874\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0830\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0691\n",
      "5470/5470 [==============================] - 9s 1ms/step - loss: 0.0298\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0298\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 20s 3ms/step - loss: 1.0621\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.1586\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.1019\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0838\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0720\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0312\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0349\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.1104\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.2709\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.1233\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.1009\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0919\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0321\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0322\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 20s 3ms/step - loss: 0.9692\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0980\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0660\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0630\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0532\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0300\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0301\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0550\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.1215\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0797\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0765\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0557\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0304\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0314\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.1366\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.1692\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0920\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0838: 0s - loss: 0.083\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0674\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0326\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0338\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.1392\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.1990\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.1079\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0900\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0935\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0396\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0450\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.9934\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0883\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0631\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0544\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0592\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0312\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0326\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.0244: 0s\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.1076\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0754\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0670\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0574\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0297\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0303\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.1313\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.1412\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0893\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0806\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0686\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0340\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0351\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.2185\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.2431\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.1066\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0816\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0757\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0320\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0338\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.8209\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0899\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0720\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0728\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0626\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0337\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0332\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.9185\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.1225\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0922\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0742\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0746\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0342\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0365\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.9762\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.1681\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.1160\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0979\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0972\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0362\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0356\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 21s 3ms/step - loss: 1.0763\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.3029\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.1509: 0s - lo\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.1268\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.1243\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0342\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0351\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.8608\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0823\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0648: 0s - los\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0637\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0608\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0329\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0303\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.9623\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.1101\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0794\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0841\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0689\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0304\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0299\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.0718\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.1638\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.1004\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0836\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0799\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0317\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0510\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.1847\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.1260\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0989\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0967\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0411\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0443\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 20s 3ms/step - loss: 0.9496\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0906\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0728\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0740\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0567\n",
      "5470/5470 [==============================] - 9s 1ms/step - loss: 0.0291\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0292\n",
      "sigmoid, 64, 96, 0.2\n",
      "0.02916168048977852\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 20s 3ms/step - loss: 0.9877\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.1043\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0735\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0599\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0637\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0316\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0324\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.0939\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.1406\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0853\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0772\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0676\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0376\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0412\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.1465\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.1933\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.1052\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0876\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0892\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0348\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0338\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.9574\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0841: 0s - loss: 0.\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0686\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0565\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0492\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0291\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0294\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 22s 3ms/step - loss: 1.0651\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.1140\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0748\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0548\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0542\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0464\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0442\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.0492\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.1292: 0s - loss\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0808\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0689: 0s - loss: 0.06\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0688\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0303\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0313\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.1404\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.1637\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0978\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0895\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0743\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0359\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0360\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.8354\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0972\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0708\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0607\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0655\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0344\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0366\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.9006\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 23s 4ms/step - loss: 0.1218\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0870\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0811\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0787\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0305\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0317\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.9970\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.1609\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 22s 4ms/step - loss: 0.1115\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0948\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0917\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0300\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0313\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 20s 3ms/step - loss: 0.9995\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.1953\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.1307\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.1119\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.1085\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0337\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0327\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 20s 3ms/step - loss: 0.8590\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0857\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0659\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0553\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0467\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0306\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0302\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.9529\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.1049\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0732\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0723\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0742\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0298\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0300\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.9743\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.1406\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 0.0917\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0783\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0786\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0319\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0321\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 22s 3ms/step - loss: 1.0706\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.1835\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.1143\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0914\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0960\n",
      "5470/5470 [==============================] - 9s 1ms/step - loss: 0.0296\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 0.0304\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 27s 4ms/step - loss: 0.9263\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0842\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0639\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0607\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0514\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0329A: 0s - loss: 0.\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0344\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.9505\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.1080\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0821\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0662\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0647\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0330\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0323\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 20s 3ms/step - loss: 1.0411\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.1366\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0909\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0721\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0717\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0304\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0299\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 20s 3ms/step - loss: 1.0823\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.1655\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.1177\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0843\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0833\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 0.0327\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0347\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 22s 3ms/step - loss: 0.9381\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 0.0905\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 0.0605\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 20s 4ms/step - loss: 0.0583\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0437\n",
      "5470/5470 [==============================] - 9s 1ms/step - loss: 0.0762\n",
      "2376/2376 [==============================] - 4s 1ms/step - loss: 0.0740\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 22s 3ms/step - loss: 0.9831\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 21s 4ms/step - loss: 0.0930\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.0728\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.0616\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 0.0562\n",
      "5470/5470 [==============================] - 10s 2ms/step - loss: 0.0331: 0\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 0.0362\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 23s 4ms/step - loss: 1.0505\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 20s 4ms/step - loss: 0.1303\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 20s 4ms/step - loss: 0.0766\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 20s 4ms/step - loss: 0.0814\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 20s 4ms/step - loss: 0.0692\n",
      "5470/5470 [==============================] - 9s 2ms/step - loss: 0.0315\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 0.0333\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 22s 3ms/step - loss: 1.0906\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - ETA: 0s - loss: 0.147 - 18s 3ms/step - loss: 0.1476\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.0904\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.0777\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.0774\n",
      "5470/5470 [==============================] - 10s 2ms/step - loss: 0.0324\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 0.0310\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 22s 3ms/step - loss: 0.8085\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0900\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0611\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0635\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0591\n",
      "5470/5470 [==============================] - 9s 1ms/step - loss: 0.0299\n",
      "2376/2376 [==============================] - 4s 1ms/step - loss: 0.0324\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 22s 3ms/step - loss: 0.8792\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.1207\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0944: 1s - l - ETA: 1s - - ETA: 0s - lo\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 26s 5ms/step - loss: 0.0871\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0735\n",
      "5470/5470 [==============================] - 9s 1ms/step - loss: 0.0374\n",
      "2376/2376 [==============================] - 4s 1ms/step - loss: 0.0404\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 25s 3ms/step - loss: 0.9917\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.1570\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 0.1078\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0978\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0862\n",
      "5470/5470 [==============================] - 9s 1ms/step - loss: 0.0340\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 0.0318\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 22s 3ms/step - loss: 1.0616\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.2376\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.1396\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.1216\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.1095A: 0s - loss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5470/5470 [==============================] - 9s 1ms/step - loss: 0.0376\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0374\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 21s 3ms/step - loss: 0.8618\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0878: 1s - l\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0657\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0623\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0633\n",
      "5470/5470 [==============================] - 9s 1ms/step - loss: 0.0299\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 0.0322A: 0s - \n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 22s 3ms/step - loss: 0.9111\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0998\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0831\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0633\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0647\n",
      "5470/5470 [==============================] - 9s 1ms/step - loss: 0.0296\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0321\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 22s 3ms/step - loss: 1.0340\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.1424\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.1001\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.0688\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 21s 4ms/step - loss: 0.0730\n",
      "5470/5470 [==============================] - 9s 1ms/step - loss: 0.0311\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0302\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 22s 3ms/step - loss: 0.9987\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 20s 4ms/step - loss: 0.1591: 0s\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 21s 4ms/step - loss: 0.1028\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0899\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 20s 4ms/step - loss: 0.0949\n",
      "5470/5470 [==============================] - 10s 2ms/step - loss: 0.0312\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 0.0323\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 24s 4ms/step - loss: 0.9148\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 20s 4ms/step - loss: 0.0801\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0591\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0528\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 0.0539\n",
      "5470/5470 [==============================] - 9s 1ms/step - loss: 0.0310\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 0.0309A: 0s\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 28s 4ms/step - loss: 0.9533\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.1076\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 20s 4ms/step - loss: 0.0709\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 20s 4ms/step - loss: 0.0606\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 20s 4ms/step - loss: 0.0675\n",
      "5470/5470 [==============================] - 10s 2ms/step - loss: 0.0301\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 0.0314\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 23s 4ms/step - loss: 1.0061\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 21s 4ms/step - loss: 0.1256\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 21s 4ms/step - loss: 0.0863\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 20s 4ms/step - loss: 0.0687\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0714\n",
      "5470/5470 [==============================] - 9s 1ms/step - loss: 0.0322\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 0.0322\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 26s 4ms/step - loss: 1.1015\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.1695\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.1079\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0874\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 0.0784\n",
      "5470/5470 [==============================] - 10s 2ms/step - loss: 0.0338\n",
      "2376/2376 [==============================] - 4s 1ms/step - loss: 0.0348\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 29s 4ms/step - loss: 0.9418\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 20s 4ms/step - loss: 0.0812\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 21s 4ms/step - loss: 0.0621\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0575\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0498\n",
      "5470/5470 [==============================] - 9s 1ms/step - loss: 0.0308\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0303\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 22s 3ms/step - loss: 0.9738\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.0951\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0785\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 24s 4ms/step - loss: 0.0584\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0583\n",
      "5470/5470 [==============================] - 9s 1ms/step - loss: 0.0300\n",
      "2376/2376 [==============================] - 4s 1ms/step - loss: 0.0297\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 23s 4ms/step - loss: 1.0601\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 0.1245\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 21s 4ms/step - loss: 0.0763\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 21s 4ms/step - loss: 0.0714\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 0.0689\n",
      "5470/5470 [==============================] - 10s 2ms/step - loss: 0.0358\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 0.0339\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 25s 4ms/step - loss: 1.1246\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 20s 4ms/step - loss: 0.1523\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 20s 4ms/step - loss: 0.0911\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0850\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 0.0732\n",
      "5470/5470 [==============================] - 9s 2ms/step - loss: 0.0315\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.0342\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.0106\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 1.0094\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 1.0097\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 1.0101\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 1.0143\n",
      "5470/5470 [==============================] - 9s 1ms/step - loss: 1.0035\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 1.0157\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.0126\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 1.0066\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 1.0087\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 1.0136\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 1.0112\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 1.0000A: 0s - loss: 0\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.9907\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.0116\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 1.0074\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 1.0086\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 1.0076\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 1.0110\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 1.0709\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.9621\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 20s 3ms/step - loss: 1.0108\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 1.0127\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 1.0106\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0146\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 1.0118\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 1.0586\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 1.1391\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.0090: 0s - los\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 1.0054\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 1.0057\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 1.0146\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 1.0123\n",
      "5470/5470 [==============================] - 9s 1ms/step - loss: 1.0038\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 0.9711\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.0105\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 1.0111\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 1.0120\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 1.0106\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 1.0079\n",
      "5470/5470 [==============================] - 9s 1ms/step - loss: 1.0001\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.9933\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.0085\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.0112\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 1.0119\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 1.0086\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 1.0072\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 1.0016\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.9771\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 23s 3ms/step - loss: 1.0083\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 1.0081\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0098\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.0111\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.0060: \n",
      "5470/5470 [==============================] - 9s 1ms/step - loss: 1.0209\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 0.9574\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 22s 3ms/step - loss: 1.0117\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 1.0126\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 1.0078\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 1.0071\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 21s 4ms/step - loss: 1.0011\n",
      "5470/5470 [==============================] - 9s 2ms/step - loss: 1.0188\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 0.9581\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 22s 3ms/step - loss: 1.0083\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 21s 4ms/step - loss: 1.0147\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 1.0118\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 1.0080\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 1.0108\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 1.0178\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 1.0578\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 23s 4ms/step - loss: 1.0066\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 1.0098\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0113\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 1.0110\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 1.0116\n",
      "5470/5470 [==============================] - 10s 2ms/step - loss: 1.0058\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 0.9679\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 23s 3ms/step - loss: 1.0095\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.0110\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 1.0081\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0070\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0106\n",
      "5470/5470 [==============================] - 11s 2ms/step - loss: 1.0069\n",
      "2376/2376 [==============================] - 5s 2ms/step - loss: 1.0280\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 23s 4ms/step - loss: 1.0090\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0104\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 1.0127\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 1.0103\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 20s 4ms/step - loss: 1.0063\n",
      "5470/5470 [==============================] - 11s 2ms/step - loss: 1.0219\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 1.0672\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 24s 4ms/step - loss: 1.0127\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0120\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 1.0134\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 21s 4ms/step - loss: 1.0087\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0121\n",
      "5470/5470 [==============================] - 9s 1ms/step - loss: 1.0008\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 1.0013\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 20s 3ms/step - loss: 1.0106\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 20s 4ms/step - loss: 1.0112\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 1.0108\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 1.0074\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 1.0127\n",
      "5470/5470 [==============================] - 9s 2ms/step - loss: 1.0014\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 1.0057\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 22s 3ms/step - loss: 1.0143\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 21s 4ms/step - loss: 1.0110\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 20s 4ms/step - loss: 1.0062\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 21s 4ms/step - loss: 1.0062\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 21s 4ms/step - loss: 1.0083\n",
      "5470/5470 [==============================] - 9s 1ms/step - loss: 1.0063\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.9671\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.0097\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 1.0106\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 15s 3ms/step - loss: 1.0123\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0099\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 1.0104\n",
      "5470/5470 [==============================] - 9s 1ms/step - loss: 1.0002A:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.9854\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 23s 3ms/step - loss: 1.0124\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 1.0111\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.0091\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 1.0170\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 1.0124\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 1.0763\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.9638\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 27s 3ms/step - loss: 1.0123\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 1.0151\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 1.0091\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0078\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0102\n",
      "5470/5470 [==============================] - 11s 2ms/step - loss: 1.0268\n",
      "2376/2376 [==============================] - 5s 2ms/step - loss: 0.9562\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 23s 3ms/step - loss: 1.0092: 2s\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.0050\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.0130\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 1.0092\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 1.0112\n",
      "5470/5470 [==============================] - 11s 2ms/step - loss: 1.0878\n",
      "2376/2376 [==============================] - 5s 2ms/step - loss: 1.1885\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 25s 4ms/step - loss: 1.0082\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 1.0130\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 20s 4ms/step - loss: 1.0124\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0071\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 1.0107\n",
      "5470/5470 [==============================] - 10s 2ms/step - loss: 1.0085\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 1.0332\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 23s 3ms/step - loss: 1.0113\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0137\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 1.0072\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0085\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 1.0085\n",
      "5470/5470 [==============================] - 10s 2ms/step - loss: 1.0002\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 0.9952\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 25s 4ms/step - loss: 1.0104\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 20s 4ms/step - loss: 1.0080\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 20s 4ms/step - loss: 1.0075\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 20s 4ms/step - loss: 1.0104\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.0122\n",
      "5470/5470 [==============================] - 10s 2ms/step - loss: 1.0101\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 1.0380\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 22s 3ms/step - loss: 1.0121\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 20s 4ms/step - loss: 1.0119\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.0069\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.0121\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 1.0107\n",
      "5470/5470 [==============================] - 10s 2ms/step - loss: 1.0000\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.9891A: 0s - los\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 21s 3ms/step - loss: 1.0090\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 1.0030\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 1.0104\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 20s 4ms/step - loss: 1.0036\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 20s 4ms/step - loss: 1.0139\n",
      "5470/5470 [==============================] - 9s 1ms/step - loss: 1.0096\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 1.0365\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 21s 3ms/step - loss: 1.0113\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.0104\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 1.0083\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 1.0106\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 1.0128\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 1.0518\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.9574\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 20s 3ms/step - loss: 1.0090\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 1.0103\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 20s 4ms/step - loss: 1.0102\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 21s 4ms/step - loss: 1.0111\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 1.0111\n",
      "5470/5470 [==============================] - 9s 2ms/step - loss: 1.0065\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 1.0269A - ETA: 0s \n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 21s 3ms/step - loss: 1.0091\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0113\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 1.0102\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 1.0086\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 1.0107\n",
      "5470/5470 [==============================] - 9s 1ms/step - loss: 1.0037\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.9714\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 21s 3ms/step - loss: 1.0090\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 21s 4ms/step - loss: 1.0101\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.0128\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 1.0033\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 1.0094\n",
      "5470/5470 [==============================] - 9s 1ms/step - loss: 1.0082\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 1.0323\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 24s 3ms/step - loss: 1.0107\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0106\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0096\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.0102\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 1.0069\n",
      "5470/5470 [==============================] - 10s 2ms/step - loss: 1.0000\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.9908\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 20s 3ms/step - loss: 1.0093\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0115\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 1.0108\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 1.0093\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 1.0097\n",
      "5470/5470 [==============================] - 9s 2ms/step - loss: 1.0012\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.9788\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 20s 3ms/step - loss: 1.0108\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0133\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 20s 4ms/step - loss: 1.0120\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0131\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.0132\n",
      "5470/5470 [==============================] - 10s 2ms/step - loss: 1.0000\n",
      "2376/2376 [==============================] - 4s 1ms/step - loss: 0.9886\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 21s 3ms/step - loss: 1.0105\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 1.0093\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 1.0103\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 1.0061\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0095\n",
      "5470/5470 [==============================] - 9s 2ms/step - loss: 1.0048\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 0.9693\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 24s 4ms/step - loss: 1.0133\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 1.0114\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0111\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 20s 4ms/step - loss: 1.0073\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.0103\n",
      "5470/5470 [==============================] - 10s 2ms/step - loss: 1.0021\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 1.0097\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 22s 3ms/step - loss: 1.0132\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 1.0133\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.0119\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 1.0125\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0134\n",
      "5470/5470 [==============================] - 12s 2ms/step - loss: 1.0178\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 1.0577\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 22s 3ms/step - loss: 1.0089\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0140\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0096\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0106\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0112\n",
      "5470/5470 [==============================] - 9s 2ms/step - loss: 1.0002\n",
      "2376/2376 [==============================] - 4s 1ms/step - loss: 0.9857A: 0s - loss:\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 22s 3ms/step - loss: 1.0101\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 1.0109\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.0102\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 1.0102\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 1.0086: 0s -\n",
      "5470/5470 [==============================] - 9s 1ms/step - loss: 1.0001\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.9869\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 24s 4ms/step - loss: 1.0085\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 20s 4ms/step - loss: 1.0093\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0093\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0103\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 20s 4ms/step - loss: 1.0070\n",
      "5470/5470 [==============================] - 10s 2ms/step - loss: 1.0014\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 1.0059\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 22s 3ms/step - loss: 1.0109\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.0095\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 20s 4ms/step - loss: 1.0127\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0070\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.0069\n",
      "5470/5470 [==============================] - 11s 2ms/step - loss: 1.0036\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 0.9717\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 24s 4ms/step - loss: 1.0136\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 1.0099\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 1.0137\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.0117\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 1.0112\n",
      "5470/5470 [==============================] - 9s 1ms/step - loss: 1.0166\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 1.0550A: 0s - loss: 1.0\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 24s 3ms/step - loss: 1.0090\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 1.0088\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 1.0092\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 1.0073\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 1.0116\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 1.0015\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.9776\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 20s 3ms/step - loss: 1.0123\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 1.0092\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 1.0129\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 1.0121\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 16s 3ms/step - loss: 1.0104\n",
      "5470/5470 [==============================] - 9s 2ms/step - loss: 1.0005\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.9826\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 25s 4ms/step - loss: 1.0075\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 1.0136\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 1.0094\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 1.0081\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0050:\n",
      "5470/5470 [==============================] - 10s 2ms/step - loss: 1.0067\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.9666\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 23s 3ms/step - loss: 1.0090\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 1.0117\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 23s 4ms/step - loss: 1.0112\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 22s 4ms/step - loss: 1.0120\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 22s 4ms/step - loss: 1.0115\n",
      "5470/5470 [==============================] - 8s 1ms/step - loss: 1.0022\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 1.0103\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 21s 3ms/step - loss: 1.0113\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 1.0102\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0090\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.0097\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 1.0083\n",
      "5470/5470 [==============================] - 9s 2ms/step - loss: 1.0002\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 0.9957\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 22s 3ms/step - loss: 1.0097\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 1.0108\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0098\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.0133\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 28s 5ms/step - loss: 1.0096\n",
      "5470/5470 [==============================] - 13s 2ms/step - loss: 1.0050\n",
      "2376/2376 [==============================] - 6s 2ms/step - loss: 1.0218\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5470/5470 [==============================] - 23s 4ms/step - loss: 1.0132\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 23s 4ms/step - loss: 1.0124\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.0101\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.0108\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 1.0086\n",
      "5470/5470 [==============================] - 13s 2ms/step - loss: 1.0056\n",
      "2376/2376 [==============================] - 5s 2ms/step - loss: 1.0237\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 26s 4ms/step - loss: 1.0077\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 21s 4ms/step - loss: 1.0080\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 22s 4ms/step - loss: 1.0135\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 22s 4ms/step - loss: 1.0090\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 24s 4ms/step - loss: 1.0100\n",
      "5470/5470 [==============================] - 14s 2ms/step - loss: 1.0003: 0s - loss\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 0.9847\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 23s 4ms/step - loss: 1.0105\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.0109\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 20s 4ms/step - loss: 1.0113\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 25s 5ms/step - loss: 1.0104\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 23s 4ms/step - loss: 1.0125\n",
      "5470/5470 [==============================] - 11s 2ms/step - loss: 1.0000\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 0.9887\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 28s 4ms/step - loss: 1.0078\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0093\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.0102\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 21s 4ms/step - loss: 1.0087\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 1.0107\n",
      "5470/5470 [==============================] - 10s 2ms/step - loss: 1.0023\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 0.9747\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 23s 3ms/step - loss: 1.0114\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0102\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0113\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0103\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 20s 4ms/step - loss: 1.0071\n",
      "5470/5470 [==============================] - 10s 2ms/step - loss: 1.0168\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 0.9589\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 27s 3ms/step - loss: 1.0092\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 21s 4ms/step - loss: 1.0111\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 23s 4ms/step - loss: 1.0133\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 1.0159\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 1.0083\n",
      "5470/5470 [==============================] - 11s 2ms/step - loss: 1.0101\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 0.9630\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 22s 3ms/step - loss: 1.0092\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 23s 4ms/step - loss: 1.0112: 0s - loss: 1.011\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 20s 4ms/step - loss: 1.0131\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0141\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0117\n",
      "5470/5470 [==============================] - 11s 2ms/step - loss: 1.0162\n",
      "2376/2376 [==============================] - 5s 2ms/step - loss: 1.0540\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 22s 3ms/step - loss: 1.0077\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 1.0119\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 21s 4ms/step - loss: 1.0122\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 20s 4ms/step - loss: 1.0078\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 1.0093\n",
      "5470/5470 [==============================] - 10s 2ms/step - loss: 1.0066\n",
      "2376/2376 [==============================] - 4s 1ms/step - loss: 1.0269A: 0s - loss: 1.028\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 23s 4ms/step - loss: 1.0101\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.0100\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 21s 4ms/step - loss: 1.0079\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0135\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 1.0102\n",
      "5470/5470 [==============================] - 9s 1ms/step - loss: 1.0015\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.9778\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 22s 3ms/step - loss: 1.0100\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 1.0114\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 21s 4ms/step - loss: 1.0087\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.0126\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0103\n",
      "5470/5470 [==============================] - 9s 1ms/step - loss: 1.0288\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.9559\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 23s 4ms/step - loss: 1.0036\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 21s 4ms/step - loss: 1.0109\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 1.0099\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.0122\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 1.0077\n",
      "5470/5470 [==============================] - 9s 2ms/step - loss: 1.0001\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 0.9939\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 21s 3ms/step - loss: 1.0100\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 1.0080\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 1.0129\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 1.0111\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 17s 3ms/step - loss: 1.0111\n",
      "5470/5470 [==============================] - 9s 1ms/step - loss: 1.0001\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.9876\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 21s 3ms/step - loss: 1.0092\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0111\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 24s 4ms/step - loss: 1.0113\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 28s 5ms/step - loss: 1.0083\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 21s 4ms/step - loss: 1.0113\n",
      "5470/5470 [==============================] - 10s 2ms/step - loss: 1.0001\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 0.9879\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 22s 3ms/step - loss: 1.0114\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0108\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0133\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 20s 4ms/step - loss: 1.0074\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 19s 3ms/step - loss: 1.0076\n",
      "5470/5470 [==============================] - 10s 2ms/step - loss: 1.0068\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.9664\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 25s 4ms/step - loss: 1.0103\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 19s 4ms/step - loss: 1.0124\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 18s 3ms/step - loss: 1.0096\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 21s 4ms/step - loss: 1.0072\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 21s 4ms/step - loss: 1.0120\n",
      "5470/5470 [==============================] - 10s 2ms/step - loss: 1.0039\n",
      "2376/2376 [==============================] - 4s 2ms/step - loss: 0.9710\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 38s 6ms/step - loss: 1.0123\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 28s 5ms/step - loss: 1.0114\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 26s 5ms/step - loss: 1.0118\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 26s 5ms/step - loss: 1.0110\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 27s 5ms/step - loss: 1.0126\n",
      "5470/5470 [==============================] - 14s 2ms/step - loss: 1.0324\n",
      "2376/2376 [==============================] - 5s 2ms/step - loss: 0.9557\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 36s 5ms/step - loss: 1.0122\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 34s 6ms/step - loss: 1.0094\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 25s 5ms/step - loss: 1.0103\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 34s 6ms/step - loss: 1.0149\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 28s 5ms/step - loss: 1.0128\n",
      "5470/5470 [==============================] - 13s 2ms/step - loss: 1.0412\n",
      "2376/2376 [==============================] - 5s 2ms/step - loss: 0.9559A: 0s - loss: 0.\n",
      "Epoch 1/5\n",
      "5470/5470 [==============================] - 31s 5ms/step - loss: 1.0129\n",
      "Epoch 2/5\n",
      "5470/5470 [==============================] - 27s 5ms/step - loss: 1.0075\n",
      "Epoch 3/5\n",
      "5470/5470 [==============================] - 27s 5ms/step - loss: 1.0098\n",
      "Epoch 4/5\n",
      "5470/5470 [==============================] - 28s 5ms/step - loss: 1.0128\n",
      "Epoch 5/5\n",
      "5470/5470 [==============================] - 26s 5ms/step - loss: 1.0085\n",
      "5470/5470 [==============================] - 13s 2ms/step - loss: 1.0148: 0s - loss: 1. - ETA: 0s - loss\n",
      "2376/2376 [==============================] - 5s 2ms/step - loss: 1.0505\n"
     ]
    }
   ],
   "source": [
    "# runs for around 4 hours! avoid running if possible!!!!\n",
    "\n",
    "# define min_loss\n",
    "min_loss = math.inf\n",
    "\n",
    "# define best hyper params to use\n",
    "best_func = None\n",
    "best_first_hidden_nodes = None\n",
    "best_second_hidden_nodes = None\n",
    "best_dropout = None\n",
    "\n",
    "for activation_func in activation_funcs:\n",
    "    for i in range(len(node_values)):\n",
    "        for j in range(len(node_values)):\n",
    "            for drop_value in drop_values:\n",
    "                \n",
    "                # define an LSTM for the normalized training data\n",
    "                cross_valid_model = Sequential()\n",
    "                # input_shape for each data input is window_size x number of features (column) in training_data\n",
    "                cross_valid_model.add(\n",
    "                    LSTM(\n",
    "                        node_values[i],\n",
    "                        activation=activation_func,\n",
    "                        input_shape=(window_size, 11),\n",
    "                        return_sequences=True\n",
    "                    )\n",
    "                )\n",
    "                cross_valid_model.add(\n",
    "                    LSTM(\n",
    "                        node_values[j],\n",
    "                        activation=activation_func,\n",
    "                        return_sequences=False\n",
    "                    )\n",
    "                )\n",
    "                cross_valid_model.add(Dropout(drop_value))\n",
    "\n",
    "                # output either standardized 0 or 1\n",
    "                cross_valid_model.add(Dense(1))\n",
    "\n",
    "                # leaving optimizer and loss funcs as-is\n",
    "                cross_valid_model.compile(optimizer='SGD', loss='mse')\n",
    "                \n",
    "                # fit data to the cross_validation model\n",
    "                cross_valid_model.fit(training_ts_generator, epochs=5)\n",
    "                \n",
    "                # training/testing loss values\n",
    "                cross_valid_train_loss = cross_valid_model.evaluate(training_ts_generator)\n",
    "                cross_valid_test_loss = cross_valid_model.evaluate(testing_ts_generator)\n",
    "                \n",
    "                if cross_valid_test_loss < min_loss:\n",
    "                    best_func = activation_func\n",
    "                    best_first_hidden_nodes = node_values[i]\n",
    "                    best_second_hidden_nodes = node_values[j]\n",
    "                    best_dropout = drop_value\n",
    "                    min_loss = cross_valid_test_loss\n",
    "                    print(f'{best_func}, {best_first_hidden_nodes}, {best_second_hidden_nodes}, {best_dropout}')\n",
    "                    print(min_loss)\n",
    "                \n",
    "# cross_valid_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid\n",
      "64\n",
      "96\n",
      "0.2\n",
      "0.02916168048977852\n"
     ]
    }
   ],
   "source": [
    "print(best_func)\n",
    "print(best_first_hidden_nodes)\n",
    "print(best_second_hidden_nodes)\n",
    "print(best_dropout)\n",
    "print(min_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
